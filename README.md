# Skip-gram Word2Vec and Negative Sampling

**NOTE:** This project is an optional excercise from [Deep Learning Nanodegree by Udacity](https://www.udacity.com/course/deep-learning-nanodegree--nd101).

My solution: [Jupyter notebook](https://nbviewer.jupyter.org/github/jscriptcoder/Skip-gram-Word2Vec/blob/master/Word2Vec.ipynb)

---

PyTorch implementation of [Word2Vec algorithm](https://arxiv.org/abs/1301.3781) using the Skip-gram architecture and [Negative Sampling](https://arxiv.org/abs/1310.4546) to improve both the quality of the vectors and the training speed. The main porpuse of this project is for me to learn about embedding words for use in Natural Language Processing.

## Readings

* [Conceptual overview](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/) of Word2Vec from Chris McCormick 
* [First Word2Vec paper](https://arxiv.org/pdf/1301.3781.pdf) from Mikolov et al.
* [Neural Information Processing Systems, paper](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) with improvements for Word2Vec also from Mikolov et al.
